{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvolutionNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAw2kAKBSj8ud+5gaRkTBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonThai2022/ML/blob/main/ConvolutionNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r99TH8aakKYr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание набора данных MNIST и подготавливаем данные"
      ],
      "metadata": {
        "id": "0SxPoYu5kXF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
        "\n",
        "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
        "with np.load(path) as data:\n",
        "  train_x = data['x_train']\n",
        "  train_y = data['y_train']\n",
        "  test_x = data['x_test']\n",
        "  test_y = data['y_test']"
      ],
      "metadata": {
        "id": "LhmYM9V_kXsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68894bb-e069-4853-976a-bd49de4dbda1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#нормализуем значения в интервал от 0 до 1\n",
        "train_x, test_x = train_x / 255.0, test_x/ 255.0"
      ],
      "metadata": {
        "id": "BBKl845Nkbk9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Преобразуем входдные данные в форму (кол-во примерв, высота, шарина, каналы)\n",
        "train_x = np.expand_dims(train_x, 1)\n",
        "test_x = np.expand_dims(test_x, 1)\n",
        "train_x = np.reshape(train_x, (60000, 28, 28, 1))\n",
        "test_x = np.reshape(test_x, (10000, 28, 28, 1))"
      ],
      "metadata": {
        "id": "wk0FKRhjq0Ho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#проверяем\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48NfqVZnrf1r",
        "outputId": "345e0922-0fc0-4a3c-b3d4-4b98e924ddda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Приводим метки к вектору one hot encoding\n",
        "train_y = tf.keras.utils.to_categorical(train_y)\n",
        "test_y = tf.keras.utils.to_categorical(test_y)"
      ],
      "metadata": {
        "id": "D90A0u2bkepH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаём валидационный набор из тестового\n",
        "validation_size = 5000\n",
        "validation_x = train_x[:validation_size, ...]\n",
        "validation_y = train_y[:validation_size]\n",
        "train_x = train_x[validation_size:, ...]\n",
        "train_y = train_y[validation_size:]"
      ],
      "metadata": {
        "id": "GxQ4sBPbqRbz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 28\n",
        "NUM_CHANNELS = 1\n",
        "NUM_LABELS = 10\n",
        "\n",
        "#Определяем кол-во неронов в слоях\n",
        "CONVOLUTION_LAYER_1_N = 32\n",
        "CONVOLUTION_LAYER_2_N = 64\n",
        "FULL_CONNECTED_LAYER_1_N = 512\n",
        "OUTPUT_LAYER_N = NUM_LABELS\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EVAL_BATCH_SIZE = 64 \n",
        "N_EPOCHS = 20\n",
        "\n",
        "train_size = 55000\n",
        "steps = train_size / BATCH_SIZE\n",
        "\n",
        "dropout_prob = 0.5\n",
        "learning_rate = 0.001\n",
        "# epsilon value for batch normalization transform\n",
        "epsilon = 1e-3"
      ],
      "metadata": {
        "id": "ISoWoqEOklMt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем архитектуру LeNet-5 по схеме \"свёртка-редукция-свёртка-редукция-полная связь-полная связь\". Здесь мы все слои инициализируем по Ксавье."
      ],
      "metadata": {
        "id": "044238Lmzr27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.disable_eager_execution() #without this will not work\n",
        "\n",
        "with tf.name_scope(\"placeholders\"):\n",
        "  x = tf.compat.v1.placeholder(tf.float32, shape=(None, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "  y = tf.compat.v1.placeholder(tf.int64, shape=(None, NUM_LABELS))\n",
        "  eval_x = tf.compat.v1.placeholder(tf.float32, shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
        "  eval_y = tf.compat.v1.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
        "  keep_prob = tf.compat.v1.placeholder(tf.float32) #вероятность того что нейрон будет работать\n",
        "  batch_size = tf.compat.v1.placeholder(tf.int64)\n",
        "with tf.name_scope(\"convolution-layer_1\"):\n",
        "  #Определение заучиваемых весов для свёрточных слоёв фильтр 5*5, глубина CONVOLUTION_LAYER_1_N\n",
        "  conv1_weights = tf.Variable(tf.compat.v1.truncated_normal([5, 5, NUM_CHANNELS, CONVOLUTION_LAYER_1_N], stddev=tf.sqrt(2/(IMAGE_SIZE + CONVOLUTION_LAYER_1_N)), dtype=tf.float32))\n",
        "  conv1_biases = tf.Variable(tf.zeros([CONVOLUTION_LAYER_1_N], dtype=tf.float32))\n",
        "  #Двумерная свёртка с заполнителем \"SAME\" (т.е. выходная карта признаков имеет тот же размер, что и вход).\n",
        "  #Примечание: {strides} - содержит шаги фильтра, форма которого соответствует конфигурации данных \n",
        "  #(по одному для каждой входной размерности): [индекс изображения, y, x, глубина].\n",
        "  x_convolution_1 = tf.nn.conv2d(x, conv1_weights, strides=[1,1,1,1], padding='SAME')\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean1, batch_var1 = tf.nn.moments(x_convolution_1,[0])\n",
        "  scale1 = tf.Variable(tf.ones([CONVOLUTION_LAYER_1_N]))\n",
        "  beta1 = tf.Variable(tf.zeros([CONVOLUTION_LAYER_1_N]))\n",
        "  x_convolution_1 = tf.nn.batch_normalization(x_convolution_1,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
        "  #Смещение и нелинейное преобразование ReLU\n",
        "  relu_1 = tf.nn.relu(tf.nn.bias_add(x_convolution_1, conv1_biases))\n",
        "  #Максимальная редукция. Спецификация размера ядра {ksize} также соответствует конфигурации данных. Здесь мы имеем окно\n",
        "  #редукции с размеров 2 и шаг размером 2.\n",
        "  pool_1 = tf.nn.max_pool(relu_1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "with tf.name_scope(\"convolution-layer_2\"):\n",
        "  #фильтр 5*5, глубина CONVOLUTION_LAYER_2_N\n",
        "  conv2_weights = tf.Variable(tf.compat.v1.truncated_normal([5, 5, CONVOLUTION_LAYER_1_N, CONVOLUTION_LAYER_2_N], stddev=tf.sqrt(2/(IMAGE_SIZE/2*CONVOLUTION_LAYER_1_N + CONVOLUTION_LAYER_2_N)), dtype=tf.float32))\n",
        "  conv2_biases = tf.Variable(tf.compat.v1.constant(0.1, shape=[CONVOLUTION_LAYER_2_N], dtype=tf.float32))\n",
        "  x_convolution_2 = tf.nn.conv2d(pool_1, conv2_weights, strides=[1,1,1,1], padding='SAME')\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean1, batch_var1 = tf.nn.moments(x_convolution_2,[0])\n",
        "  scale1 = tf.Variable(tf.ones([CONVOLUTION_LAYER_2_N]))\n",
        "  beta1 = tf.Variable(tf.zeros([CONVOLUTION_LAYER_2_N]))\n",
        "  x_convolution_2 = tf.nn.batch_normalization(x_convolution_2,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
        "  ###########################\n",
        "  relu_2 = tf.nn.relu(tf.nn.bias_add(x_convolution_2, conv2_biases))\n",
        "  pool_2 = tf.nn.max_pool(relu_2, ksize=[1,2,2,1], strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "with tf.name_scope(\"reshape_to_flat\"):\n",
        "  #Реформировать кубоид с картой признаков в двумерную матрицу, чтобы подать её в полносвязные слои\n",
        "  pool_shape = pool_2.get_shape().as_list()\n",
        "  #Здесь необходимо подставлять размер батча чтобы работала функция evaluate\n",
        "  reshape = tf.reshape(pool_2, [batch_size, pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
        "\n",
        "with tf.name_scope(\"full_connected_layer_1\"):\n",
        "  #Полносвязный слой. Примечание: операция '+' автоматически транслирует смещения\n",
        "  #Здесь мы прописываем напрямую входные измерения, лучше поменять!\n",
        "  fc1_weights = tf.Variable(tf.compat.v1.truncated_normal([12544, FULL_CONNECTED_LAYER_1_N], stddev=tf.sqrt(2/(12544 + FULL_CONNECTED_LAYER_1_N)), dtype=tf.float32))\n",
        "  fc1_biases = tf.Variable(tf.compat.v1.constant(0.1, shape=[FULL_CONNECTED_LAYER_1_N], dtype=tf.float32))\n",
        "  hidden_1 = tf.nn.relu(tf.compat.v1.matmul(reshape, fc1_weights) + fc1_biases)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean1, batch_var1 = tf.nn.moments(hidden_1,[0])\n",
        "  scale1 = tf.Variable(tf.ones([FULL_CONNECTED_LAYER_1_N]))\n",
        "  beta1 = tf.Variable(tf.zeros([FULL_CONNECTED_LAYER_1_N]))\n",
        "  hidden_1 = tf.nn.batch_normalization(hidden_1,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
        "  #Добавить отсев 50% только во время тр-ки. Отсев также шкалирует активациии, что позволяет обходиться без перешкалирования\n",
        "  # во время оценивания модели\n",
        "  hidden_1 = tf.nn.dropout(hidden_1, keep_prob)\n",
        "\n",
        "with tf.name_scope(\"output_layer\"):\n",
        "  fc2_weights = tf.Variable(tf.compat.v1.truncated_normal([FULL_CONNECTED_LAYER_1_N, NUM_LABELS], stddev=tf.sqrt(2/(FULL_CONNECTED_LAYER_1_N + OUTPUT_LAYER_N)), dtype=tf.float32))\n",
        "  fc2_biases = tf.Variable(tf.compat.v1.constant(0.1, shape=[OUTPUT_LAYER_N], dtype=tf.float32))\n",
        "  y_logit = tf.nn.relu(tf.compat.v1.matmul(hidden_1, fc2_weights) + fc2_biases)\n",
        "  prediction = tf.nn.softmax(logits = y_logit) \n",
        "  prediction_class = tf.argmax(prediction,1)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "  # Compute the cross-entropy term for each datapoint\n",
        "  entropy = tf.nn.softmax_cross_entropy_with_logits(logits = y_logit, labels=y)\n",
        "  # Sum all contributions\n",
        "  l = tf.reduce_mean(entropy)\n",
        "\n",
        "with tf.name_scope(\"optim\"):\n",
        "  optim = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(l)  "
      ],
      "metadata": {
        "id": "hGtOpi9tQgrx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тренировка архитектуры LeNet-5"
      ],
      "metadata": {
        "id": "OWKkLtbGMvda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  for epoch in range(N_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    pos = 0\n",
        "    for i in range(int(steps)):\n",
        "       batch_x = train_x[pos:pos+BATCH_SIZE:]\n",
        "       batch_y = train_y[pos:pos+BATCH_SIZE:]\n",
        "       _, loss = sess.run([optim, l], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_prob, batch_size: BATCH_SIZE})\n",
        "       pos += BATCH_SIZE\n",
        "       epoch_loss += loss\n",
        "       #if(i % 1000 == 0):\n",
        "       #  print(\"потеря: %f\" % loss)\n",
        "    print('Epoch ',epoch, ' completed out of ',N_EPOCHS, 'loss: ', epoch_loss )\n",
        "    # calculate the accuracy of the acuracy of this model\n",
        "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
        "    # Evaluate the accuracy on the Test data\n",
        "    print ('Accuracy on test data is: ', accuracy.eval({x: test_x, y: test_y, keep_prob: 0, batch_size: 10000}))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WZAydyOi99Z",
        "outputId": "82edb2bd-751f-4964-ee7c-d60d36ef7a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  13  completed out of  20 loss:  4.1574933008978405\n",
            "Accuracy on test data is:  0.9895\n",
            "Epoch  14  completed out of  20 loss:  2.320031882672083\n",
            "Accuracy on test data is:  0.9909\n",
            "Epoch  15  completed out of  20 loss:  3.9939583827808747\n",
            "Accuracy on test data is:  0.9902\n",
            "Epoch  16  completed out of  20 loss:  4.051381335977453\n",
            "Accuracy on test data is:  0.9903\n",
            "Epoch  17  completed out of  20 loss:  3.4134848273865828\n",
            "Accuracy on test data is:  0.9905\n",
            "Epoch  18  completed out of  20 loss:  3.6151188965686742\n",
            "Accuracy on test data is:  0.992\n"
          ]
        }
      ]
    }
  ]
}