{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntonThai2022/ML/blob/main/FullyConnectedNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9xEYh8D8RhM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTI5jsQ3PUk0"
      },
      "source": [
        "Скачивание набора данных MNIST и подготавливаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TivJbwR1OfM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a45bf2a4-4985-465d-95e2-298b854cbf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
        "\n",
        "path = tf.keras.utils.get_file('mnist.npz', DATA_URL)\n",
        "with np.load(path) as data:\n",
        "  train_x = data['x_train']\n",
        "  train_y = data['y_train']\n",
        "  test_x = data['x_test']\n",
        "  test_y = data['y_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-jKK3RoSJ0I"
      },
      "outputs": [],
      "source": [
        "#нормализуем значения в интервал от 0 до 1\n",
        "train_x, test_x = train_x / 255.0, test_x/ 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sQsGk88QlSj"
      },
      "outputs": [],
      "source": [
        "#Выпрямляем изображение\n",
        "train_x = train_x.reshape(60000, 784)\n",
        "test_x = test_x.reshape(10000, 784)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Приводим метки к вектору one hot encoding\n",
        "train_y = tf.keras.utils.to_categorical(train_y)\n",
        "test_y = tf.keras.utils.to_categorical(test_y)\n",
        "\n"
      ],
      "metadata": {
        "id": "iFvqqmgWZr41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huaI7U-m1ZX3"
      },
      "source": [
        "Определяем необходимые переменные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to1K29Rx1dvv"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "steps = 60000 / batch_size\n",
        "n_epochs = 20\n",
        "dropout_prob = 0.5\n",
        "learning_rate = 0.001\n",
        "# epsilon value for batch normalization transform\n",
        "epsilon = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1HqLDw--IQ-"
      },
      "source": [
        "Создаём модель полносвязной сети.\n",
        "При инициализации весов используем инизиализацию Ксавье: tf.compat.v1.random_normal(shape=[input, n_hidden], stddev=tf.sqrt(2/(input+n_hidden)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBKKh-XL-aS5"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution() #without this will not work\n",
        "\n",
        "input = 784\n",
        "n_hidden_1 = 1024\n",
        "n_hidden_2 = 1024\n",
        "n_hidden_3 = 1024\n",
        "n_hidden_4 = 1024\n",
        "n_hidden_5 = 1024\n",
        "n_output = 10\n",
        "\n",
        "with tf.name_scope(\"placeholders\"):\n",
        "  x = tf.compat.v1.placeholder(tf.float32, (None, input))\n",
        "  y = tf.compat.v1.placeholder(tf.float32, (None, n_output))\n",
        "  keep_prob = tf.compat.v1.placeholder(tf.float32) #вероятность того что нейрон будет работать\n",
        "with tf.name_scope(\"hidden-layer_1\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[input, n_hidden_1], stddev=tf.sqrt(2/(input+n_hidden_1))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden_1,)))\n",
        "  x_hidden_1 = tf.nn.relu(tf.compat.v1.matmul(x, W) + b)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean1, batch_var1 = tf.nn.moments(x_hidden_1,[0])\n",
        "  scale1 = tf.Variable(tf.ones([1024]))\n",
        "  beta1 = tf.Variable(tf.zeros([1024]))\n",
        "  x_hidden_1 = tf.nn.batch_normalization(x_hidden_1,batch_mean1,batch_var1,beta1,scale1,epsilon)\n",
        "  ##############  \n",
        "  x_hidden_1 = tf.nn.dropout(x_hidden_1, keep_prob)\n",
        "with tf.name_scope(\"hidden-layer_2\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[n_hidden_1, n_hidden_2], stddev=tf.sqrt(2/(n_hidden_1+n_hidden_2))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden_2,)))\n",
        "  x_hidden_2 = tf.nn.relu(tf.compat.v1.matmul(x_hidden_1, W) + b)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean2, batch_var2 = tf.nn.moments(x_hidden_2,[0])\n",
        "  scale2 = tf.Variable(tf.ones([1024]))\n",
        "  beta2 = tf.Variable(tf.zeros([1024]))\n",
        "  x_hidden_2 = tf.nn.batch_normalization(x_hidden_2,batch_mean2,batch_var2,beta2,scale2,epsilon)\n",
        "  ##############  \n",
        "  x_hidden_2 = tf.nn.dropout(x_hidden_2, keep_prob)  \n",
        "with tf.name_scope(\"hidden-layer_3\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[n_hidden_2, n_hidden_3], stddev=tf.sqrt(2/(n_hidden_2+n_hidden_3))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden_3,)))\n",
        "  x_hidden_3 = tf.nn.relu(tf.compat.v1.matmul(x_hidden_2, W) + b)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean3, batch_var3 = tf.nn.moments(x_hidden_3,[0])\n",
        "  scale3 = tf.Variable(tf.ones([1024]))\n",
        "  beta3 = tf.Variable(tf.zeros([1024]))\n",
        "  x_hidden_3 = tf.nn.batch_normalization(x_hidden_3,batch_mean3,batch_var3,beta3,scale3,epsilon)\n",
        "  ##############  \n",
        "  x_hidden_3 = tf.nn.dropout(x_hidden_3, keep_prob) \n",
        "with tf.name_scope(\"hidden-layer_4\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[n_hidden_3, n_hidden_4], stddev=tf.sqrt(2/(n_hidden_3+n_hidden_4))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden_4,)))\n",
        "  x_hidden_4 = tf.nn.relu(tf.compat.v1.matmul(x_hidden_3, W) + b)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean4, batch_var4 = tf.nn.moments(x_hidden_4,[0])\n",
        "  scale4 = tf.Variable(tf.ones([1024]))\n",
        "  beta4 = tf.Variable(tf.zeros([1024]))\n",
        "  x_hidden_4 = tf.nn.batch_normalization(x_hidden_4,batch_mean4,batch_var4,beta4,scale4,epsilon)\n",
        "  ##############  \n",
        "  x_hidden_4 = tf.nn.dropout(x_hidden_4, keep_prob) \n",
        "with tf.name_scope(\"hidden-layer_5\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[n_hidden_4, n_hidden_5], stddev=tf.sqrt(2/(n_hidden_4+n_hidden_5))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_hidden_5,)))\n",
        "  x_hidden_5 = tf.nn.relu(tf.compat.v1.matmul(x_hidden_4, W) + b)\n",
        "  #нормализация по мини-батчам\n",
        "  batch_mean5, batch_var5 = tf.nn.moments(x_hidden_5,[0])\n",
        "  scale5 = tf.Variable(tf.ones([1024]))\n",
        "  beta5 = tf.Variable(tf.zeros([1024]))\n",
        "  x_hidden_5 = tf.nn.batch_normalization(x_hidden_5,batch_mean5,batch_var5,beta5,scale5,epsilon)\n",
        "  ##############  \n",
        "  x_hidden_5 = tf.nn.dropout(x_hidden_5, keep_prob) \n",
        "with tf.name_scope(\"output\"):\n",
        "  W = tf.compat.v1.Variable(tf.compat.v1.random_normal(shape=[n_hidden_5, n_output], stddev=tf.sqrt(2/(n_hidden_5+n_output))))\n",
        "  b = tf.compat.v1.Variable(tf.compat.v1.random_normal((n_output,)))\n",
        "  y_logit = tf.compat.v1.matmul(x_hidden_5, W)+ b\n",
        "  prediction = tf.nn.softmax(logits = y_logit) \n",
        "  prediction_class = tf.argmax(prediction,1)\n",
        "with tf.name_scope(\"loss\"):\n",
        "  # Compute the cross-entropy term for each datapoint\n",
        "  entropy = tf.nn.softmax_cross_entropy_with_logits(logits = y_logit, labels=y)\n",
        "  # Sum all contributions\n",
        "  l = tf.reduce_mean(entropy)\n",
        "\n",
        "with tf.name_scope(\"optim\"):\n",
        "  optim = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOGYCWKHJntd"
      },
      "source": [
        "Тренируем на минибатчах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6R-oxWDJ1Jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e275e1b0-b1ff-40ad-88d5-a176b317d03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0  completed out of  20 loss:  313.37243124097586\n",
            "Accuracy on test data is:  0.9515\n",
            "Epoch  1  completed out of  20 loss:  143.98742599971592\n",
            "Accuracy on test data is:  0.9615\n",
            "Epoch  2  completed out of  20 loss:  113.00598432775587\n",
            "Accuracy on test data is:  0.9656\n",
            "Epoch  3  completed out of  20 loss:  98.09630787186325\n",
            "Accuracy on test data is:  0.9683\n",
            "Epoch  4  completed out of  20 loss:  90.40385969355702\n",
            "Accuracy on test data is:  0.9715\n",
            "Epoch  5  completed out of  20 loss:  82.04129243828356\n",
            "Accuracy on test data is:  0.9729\n",
            "Epoch  6  completed out of  20 loss:  73.83985717175528\n",
            "Accuracy on test data is:  0.9728\n",
            "Epoch  7  completed out of  20 loss:  71.79564460832626\n",
            "Accuracy on test data is:  0.9756\n",
            "Epoch  8  completed out of  20 loss:  64.89686195738614\n",
            "Accuracy on test data is:  0.977\n",
            "Epoch  9  completed out of  20 loss:  62.05405300948769\n",
            "Accuracy on test data is:  0.9768\n",
            "Epoch  10  completed out of  20 loss:  57.44163385615684\n",
            "Accuracy on test data is:  0.9796\n",
            "Epoch  11  completed out of  20 loss:  55.3614081828855\n",
            "Accuracy on test data is:  0.9786\n",
            "Epoch  12  completed out of  20 loss:  51.71066933311522\n",
            "Accuracy on test data is:  0.9775\n",
            "Epoch  13  completed out of  20 loss:  48.30061150621623\n",
            "Accuracy on test data is:  0.9813\n",
            "Epoch  14  completed out of  20 loss:  45.83345929370262\n",
            "Accuracy on test data is:  0.9801\n",
            "Epoch  15  completed out of  20 loss:  44.721235414152034\n",
            "Accuracy on test data is:  0.9804\n",
            "Epoch  16  completed out of  20 loss:  39.08718800963834\n",
            "Accuracy on test data is:  0.9805\n",
            "Epoch  17  completed out of  20 loss:  40.711978778010234\n",
            "Accuracy on test data is:  0.9806\n",
            "Epoch  18  completed out of  20 loss:  36.297651005443186\n",
            "Accuracy on test data is:  0.9826\n",
            "Epoch  19  completed out of  20 loss:  36.719295151298866\n",
            "Accuracy on test data is:  0.9822\n"
          ]
        }
      ],
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_loss = 0\n",
        "    pos = 0\n",
        "    for i in range(int(steps)):\n",
        "       batch_x = train_x[pos:pos+batch_size:]\n",
        "       batch_y = train_y[pos:pos+batch_size:]\n",
        "       _, loss = sess.run([optim, l], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_prob})\n",
        "       pos += batch_size\n",
        "       epoch_loss += loss\n",
        "       #if(i % 1000 == 0):\n",
        "       #  print(\"потеря: %f\" % loss)\n",
        "    print('Epoch ',epoch, ' completed out of ',n_epochs, 'loss: ', epoch_loss )\n",
        "    # calculate the accuracy of the acuracy of this model\n",
        "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
        "    # Evaluate the accuracy on the Test data\n",
        "    print ('Accuracy on test data is: ', accuracy.eval({x: test_x, y: test_y, keep_prob: 0}))  \n",
        "  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "FullyConnectedNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPvdVu0HiJ0yCZvXJmLY6xg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}